{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relax Data Science Challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the data of the users and their engagement data. We must find the most important factors contributing to user adoption.\n",
    "Let us first read in the user-engagement data and filter those users who have accessed the product atleast thrice on three separate days in a seven day period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries\n",
    "\n",
    "As this is an investigative study, we will import and use libraries as and when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the user-engagement data and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>visited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-04-22 03:53:30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-15 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-29 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-09 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-25 03:45:04</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time_stamp  user_id  visited\n",
       "0  2014-04-22 03:53:30        1        1\n",
       "1  2013-11-15 03:45:04        2        1\n",
       "2  2013-11-29 03:45:04        2        1\n",
       "3  2013-12-09 03:45:04        2        1\n",
       "4  2013-12-25 03:45:04        2        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_engagement = pd.read_csv('../relax_challenge/takehome_user_engagement.csv')\n",
    "user_engagement.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with dates\n",
    "We convert the 'timestamp' column into the appropriate datatype and also group the engagement data by date(as multiple times within the same day counts as once towards adoption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_engagement['time_stamp'] = user_engagement['time_stamp'].astype('datetime64')\n",
    "user_engagement['date'] = user_engagement['time_stamp'].dt.date\n",
    "user_engagement = user_engagement.groupby(['date','user_id']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demarcate Adopted users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = user_engagement.sort_values(['user_id', 'date']) - user_engagement.sort_values(['user_id', 'date']).shift(-2)\n",
    "adopted_users = user_engagement.sort_values(['user_id', 'date'])[(condition['date'].dt.days >= -7) & (condition['user_id'] == 0)]['user_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the users data and convert dates to appropriate datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv('../relax_challenge/takehome_users.csv', encoding = 'latin-1')\n",
    "user_data['creation_time'] = user_data['creation_time'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the features given in the dataset, we add an additional feature which indicates if the user was invited by an adopted user.\n",
    "We then drop other details such as personal information so that they will not interfere in our predictive modelling. We also onehot encode the 'creation_source' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data['invited_by_adopted_user'] = user_data['invited_by_user_id'].map(lambda x: int(x in adopted_users))\n",
    "user_data = pd.concat([user_data, pd.get_dummies(user_data['creation_source'])], axis = 1)\n",
    "user_data['adopted_user'] = user_data['object_id'].map(lambda x: int(x in adopted_users))\n",
    "user_data.drop(['name', 'email', 'creation_source', 'last_session_creation_time', 'object_id', 'invited_by_user_id', 'org_id', 'creation_time'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      2581\n",
      "           1       0.00      0.00      0.00       419\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.43      0.50      0.46      3000\n",
      "weighted avg       0.74      0.86      0.80      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      2581\n",
      "           1       0.00      0.00      0.00       419\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.43      0.50      0.46      3000\n",
      "weighted avg       0.74      0.86      0.80      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      2581\n",
      "           1       0.00      0.00      0.00       419\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.43      0.50      0.46      3000\n",
      "weighted avg       0.74      0.86      0.80      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      2581\n",
      "           1       0.00      0.00      0.00       419\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.43      0.50      0.46      3000\n",
      "weighted avg       0.74      0.86      0.80      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      2581\n",
      "           1       0.00      0.00      0.00       419\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.43      0.50      0.46      3000\n",
      "weighted avg       0.74      0.86      0.80      3000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      2581\n",
      "           1       0.00      0.00      0.00       419\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.43      0.50      0.46      3000\n",
      "weighted avg       0.74      0.86      0.80      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyasiddharthdash/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/satyasiddharthdash/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/satyasiddharthdash/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/satyasiddharthdash/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/satyasiddharthdash/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/satyasiddharthdash/ml/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "X = user_data.values[:,:-1]\n",
    "y = user_data.values[:,-1]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle = True)\n",
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "for C in Cs:\n",
    "    clf = LogisticRegression(C = C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    Y_pred_valid = clf.predict(X_valid)\n",
    "    print(classification_report(y_valid, Y_pred_valid))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "Y_pred_valid = clf.predict(X_valid)\n",
    "print(classification_report(y_valid, Y_pred_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import imblearn.over_sampling as imb\n",
    "oversample = imb.SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52      2602\n",
      "           1       0.55      0.64      0.59      2570\n",
      "\n",
      "    accuracy                           0.56      5172\n",
      "   macro avg       0.56      0.56      0.55      5172\n",
      "weighted avg       0.56      0.56      0.55      5172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52      2602\n",
      "           1       0.55      0.64      0.59      2570\n",
      "\n",
      "    accuracy                           0.56      5172\n",
      "   macro avg       0.56      0.56      0.55      5172\n",
      "weighted avg       0.56      0.56      0.55      5172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52      2602\n",
      "           1       0.55      0.64      0.59      2570\n",
      "\n",
      "    accuracy                           0.56      5172\n",
      "   macro avg       0.56      0.56      0.55      5172\n",
      "weighted avg       0.56      0.56      0.55      5172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52      2602\n",
      "           1       0.55      0.64      0.59      2570\n",
      "\n",
      "    accuracy                           0.56      5172\n",
      "   macro avg       0.56      0.56      0.55      5172\n",
      "weighted avg       0.56      0.56      0.55      5172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52      2602\n",
      "           1       0.55      0.64      0.59      2570\n",
      "\n",
      "    accuracy                           0.56      5172\n",
      "   macro avg       0.56      0.56      0.55      5172\n",
      "weighted avg       0.56      0.56      0.55      5172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.48      0.52      2602\n",
      "           1       0.55      0.64      0.59      2570\n",
      "\n",
      "    accuracy                           0.56      5172\n",
      "   macro avg       0.56      0.56      0.55      5172\n",
      "weighted avg       0.56      0.56      0.55      5172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle = True)\n",
    "Cs = [0.001, 0.1, 1, 10, 100]\n",
    "for C in Cs:\n",
    "    clf = LogisticRegression(C = C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    Y_pred_valid = clf.predict(X_valid)\n",
    "    print(classification_report(y_valid, Y_pred_valid))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "Y_pred_valid = clf.predict(X_valid)\n",
    "print(classification_report(y_valid, Y_pred_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84      1037\n",
      "           1       0.17      0.24      0.20       163\n",
      "\n",
      "    accuracy                           0.74      1200\n",
      "   macro avg       0.52      0.53      0.52      1200\n",
      "weighted avg       0.78      0.74      0.76      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyasiddharthdash/ml/lib/python3.7/site-packages/sklearn/tree/_classes.py:327: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "X = user_data.values[:,:-1]\n",
    "y = user_data.values[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size = 0.1)\n",
    "clf = DecisionTreeClassifier()\n",
    "param_grid = {\"criterion\": [\"gini\", \"entropy\"], \n",
    "                       \"max_depth\": [1,2,.3,4,5,6,None], \n",
    "                       \"max_features\": [\"sqrt\", \"log2\", None], \n",
    "                       \"random_state\" : [5], \n",
    "                       \"class_weight\" : [\"balanced\", {0: 1, 1:10}, {0:1, 1:15}], \"presort\": [True]}\n",
    "gs = GridSearchCV(estimator = clf, \n",
    "                   param_grid = param_grid, \n",
    "                   n_jobs = 2, \n",
    "                   cv = None)\n",
    "gs.fit(X_train, y_train)\n",
    "best_clf = gs.best_estimator_\n",
    "Y_test = best_clf.predict(X_test)\n",
    "print(classification_report(y_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz.backend as be\n",
    "from dtreeviz.trees import *\n",
    "viz = dtreeviz(best_clf, \n",
    "               X, \n",
    "               y,\n",
    "               target_name = 'Adopted User',\n",
    "               feature_names = user_data.columns[:-1],\n",
    "               class_names=[\"Adopted User\", \"Not adopted user\"],\n",
    "              fancy=False )\n",
    "\n",
    "viz.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "?viz.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitd2f58a4918ec49a6be3098fab8dcc577"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
